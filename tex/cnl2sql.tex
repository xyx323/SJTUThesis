%# -*- coding: utf-8-unix -*-
% !TEX program = xelatex
% !TEX root = ../thesis.tex
% !TEX encoding = UTF-8 Unicode
%%==================================================
%% chapter02.tex for SJTU Master Thesis
%% based on CASthesis
%% modified by wei.jianwen@gmail.com
%% Encoding: UTF-8
%%==================================================

\chapter{基于多任务学习的NL2SQL生成}
\label{chap:cnl2sql}

\section{研究问题}

在第\ref{chap:enl2sql}章中，我们结合了深度学习与深度强化学习来解决NL2SQL的问题。
但是，目前的所有研究都只是将英文的自然语言问题转换为机器可执行的SQL查询语句，而没有涉及到其他语种的自然语言。
在本章中，我们主要的研究问题就是研究如何进一步提升NL2SQL任务中英文自然语言生成SQL查询语句的准确性以及如何同时解决中文自然语言问题转换为SQL查询语句的问题。
其中最具挑战的地方就是怎样将中文自然语言转换为英文自然语言的任务与英文自然语言生成SQL查询语句有机地结合起来。

基于多任务学习的NL2SQL生成的模型的总输入表示为$x$，其包含由单词$w_{i}$组成的自然语言问题$w$以及由列名$c_{j}$组成的数据库单张表的模式 $c$（其中，列名$c_{j}$可由单个或多个单词组成）。
模型的输出为一条可执行的SQL查询语句$y$以及其在数据库中执行的结果$r$。
我们通过给出NL2SQL任务中一个典型的例子（如图\ref{fig:nl2sqlexample}所示）来进一步说明。

\begin{figure}[!htp]
    \centering
    \includegraphics[width=17cm]{example/cnl2sql.pdf}
    \bicaption[这里将出现在插图索引中]
      {中文自然语言问题生成SQL查询语句的示例}
      {English caption}
    \label{fig:cnl2sqlexample}
  \end{figure}

在图\ref{fig:cnl2sqlexample}中，$w$代表中文自然语言问题“芝加哥的威利斯大厦有多少层？”，其中$w_0,w_1,w_2,...$分别代表单词“芝加哥”、“的”、“威利斯大厦”等。
$c$代表数据库表的模式“Name Location Height(ft) Floor Year”，其中$c_0,c_1,c_2,...$分别代表单词“Name”、“Location”、“Height(ft)”等。
总输入$x$代表由$w$和$c$组成的集合。
在将中文的自然语言问题转换为SQL查询语句的过程中会生成中间结果“How  many  floors  does  the  Willis  Tower  in  Chicago  have?”，即将中文翻译为英文，记作$m$。
模型的输出$y$在该示例中对应的SQL查询语句为“SELECT ‘Floor’ WHERE Name=‘Willis Tower’ AND Location=‘Chicago’”，其在数据库中执行的结果$r$为108。

基于多任务学习的NL2SQL生成的主要任务是要在将中文翻译为英文的同时，理解中文自然语言语句的意图并在一次交互的状态下将意图映射到SQL查询语句上，
即在知道数据库表模式$c$的状态下，将用户输入的自然语言问题$w$最终转换为一条SQL查询语句$y$并得到数据库执行结果$r$。

\section{相关技术}
\subsection{迁移学习与多任务学习}
\subsection{元学习}

\section{解决方案}

在本节中，我们会介绍如何把中文自然语言翻译为英文自然语言任务与英文自然语言生成SQL查询语句任务进行统一，之后提出一种基于多任务学习的神经网络结构并解释说明为何这样的网络结构可以有效地解决中文自然语言问题生成SQL查询语句问题。

\subsection{TCR模板}

首先，中文自然语言问题生成SQL查询语句可以被划分为两个子任务：中文自然语言翻译为英文自然语言任务和英文自然语言问题生成SQL查询语句任务。
在第\ref{chap:enl2sql}章中，本文提出的基于深度强化学习的解决方案已经可以有效的解决NL2SQL问题，并且在WikiSQL数据集和spider数据集上有优异的表现。
所以，我们会有一个非常自然的想法，就是直接使用翻译工具或翻译软件将中文的自然语言问题直接转换为英文自然语言问题，之后再使用\ref{enl2sql:zqjxqmx}节中提出的增强解析器模型将英文自然语言转换为SQL查询语句。
但是，这种方法会遇到许多问题，如图\ref{fig:cnl2sqlproblem}所示（其中中文自然语言均使用谷歌翻译自动翻译为英文自然语言）：

\begin{figure}[!htp]
    \centering
    \includegraphics[width=17cm]{example/cnl2sqlproblem.pdf}
    \bicaption[这里将出现在插图索引中]
      {中文自然语言问题生成SQL查询语句的错误示例}
      {English caption}
    \label{fig:cnl2sqlproblem}
  \end{figure}

从图\ref{fig:cnl2sqlproblem}中的示例一可以看出，数据库表模式中的列名为“College”,而谷歌翻译将中文“大学”翻译为“University”。
而示例二中的“层”翻译为了“layers”而不是“floors”。
形如这一类的问题会给SQL的生成带来很大的问题。

究其原因，从中文自然语言到英文自然语言再到SQL查询语句这一过程是一个紧密关联的过程，如果将其拆分为两个部分，则翻译的过程中将会丢失大量的数据库、SQL语言本身的很多信息。
所以，我们提出TCR模板用以将这两个独立的任务有机的统一起来，并在作为下节中的多任务学习网络的输入。

\begin{figure}[!htp]
    \centering
    \includegraphics[width=17cm]{example/tcr.pdf}
    \bicaption[这里将出现在插图索引中]
      {TCR模板（任务-内容-结果模板）}
      {English caption}
    \label{fig:cnl2sqltcr}
  \end{figure}

如图\ref{fig:cnl2sqltcr}所示，我们将中文自然语言翻译为英文自然语言任务和英文自然语言生成SQL查询语句任务使用TCR模板（任务-内容-结果）进行统一。
例如，“告诉我南澳有哪些注意要点”先翻译为英文，其任务为“What is the translation from Chinese to English?”（意为需要执行的任务是中文到英文的翻译），内容为“告诉我南澳有哪些注意要点”（指在表明所需要翻译的中文自然语句是什么），结果为“Tell me what the notes are for South Australia”。
而后再将“Tell me what the notes are for South Australia”转换为SQL查询语句，其任务为“What is the translation from English to SQL?”（意为需要执行的任务是英文到SQL的生成），内容为“The table has column names… Tell me what the notes are for South Australia”（指在表明所需的信息由表的列名以及英文自然语言的查询构成），结果为“SELECT notes from table WHERE ‘Current Slogan’ =‘South Australia”，从而完成生成的全过程。

TCR模板将两个任务的模式进行了统一，使得网络模型的输入和输出得以一致，而不用设计两个不同的网络结构来处理这两个任务。
在后文中，我们还将这个模板用于自然语言处理领域中的很多任务，详见\ref{cnl2sql:syyfx}节。

\subsection{多任务网络}

由于我们会使用TCR模板对每个任务进行统一并且在训练阶段采用联合训练的方法，我们把所使用的神经网络的结构称为多任务网络，如图\ref{fig:cnl2sqlnet}所示。
近几年来，有许多研究者所研究的问答模型的和我们的模型比较类似，但是通常的问答模型往往假设模型得到的答案的片段是可以直接从上下文信息中复制，但是我们的多任务网络适用于更一般的问答场景。
在之前的TCR模板中，任务对应于问答模型中的问题，内容对应于问答模型中的上下文，而结果对应于问答模型中的回答。
由于TCR模板中的任务往往包含限制搜索空间的非常关键的信息，我们会在多任务网络中使用了协同注意力机制[！！引用！！]并加以拓展，从而使得对模型输入的表示更加丰富。
此外，多任务网络还采用了指针机制[！！引用！！]，并将其改造为分层的多指针生成器，使得网络能够直接从任务和内容中进行文本的复制。

\begin{figure}[!htp]
  \centering
  \includegraphics[width=17cm]{example/cnl2sqlnet.pdf}
  \bicaption[这里将出现在插图索引中]
    {多任务网络}
    {English caption}
  \label{fig:cnl2sqlnet}
\end{figure}

在训练阶段，多任务网络的输入序列会有三个部分，分别为：由$l$个单词构成的内容$c$；
由$m$个单词构成的任务$q$；由$n$个单词构成的结果$a$。
其中，每个输入序列都是一个矩阵，矩阵中的第$i$行对应于序列中的第$i$个词的嵌入(embedding)（可为字符向量或词向量），维度为$d_{emb}$。有公式\ref{cnl2sqleq:drwwl1}：
\begin{equation}
    \label{cnl2sqleq:drwwl1}
    C \in \mathbb{R}^{l \times d_{emb}} \qquad Q \in \mathbb{R}^{m \times d_{emb}} \qquad A \in \mathbb{R}^{n \times d_{emb}}
  \end{equation}

接下来，我们将分别详细介绍多任务网络中的编码器和解码器。

\subsection{编码器}
\label{cnl2sql:encoder}

编码器将内容矩阵$C$、任务矩阵$Q$和结果矩阵$A$作为输入，
使用深度栈式循环神经网络[！！引用！！]，结合协同注意力机制(coattention)[！！引用！！]和自注意力机制(self-attention)[！！引用！！],
来生成编码阶段的最终的表示$C_{fin} \in \mathbb{R}^{l \times d}$和$Q_{fin} \in \mathbb{R}^{m \times d}$（即编码之后的内容信息和任务信息），
并且还可以让网络学习到他们之间的局部和全局的相互依赖关系。
在下文中，我们将对编码器的每个阶段进行进一步的说明。

\textbf{单独编码阶段}

首先使用一个简单的线性层将输入矩阵投射到$d$维的空间上，如公式\ref{cnl2sqleq:encoder1}。
\begin{equation}
    \label{cnl2sqleq:encoder1}
    CW_1 = C_{proj} \in \mathbb{R}^{l \times d} \qquad QW_1 = Q_{proj} \in \mathbb{R}^{m \times d} 
  \end{equation}
这些投射之后的表示将被输入给一个共享的双向长短期记忆网络(bi-LSTM)[！！引用！！][注释]，如公式\ref{cnl2sqleq:encoder2}。
\begin{equation}
    \label{cnl2sqleq:encoder2}
    BILSTM_{ind}(C_{proj}) = C_{ind} \in \mathbb{R}^{l \times d} \qquad BILSTM_{ind}(Q_{proj}) = Q_{ind} \in \mathbb{R}^{l \times d}
\end{equation}

\textbf{对准阶段}

在获得协同注意力的表达之前，我们需要先对每个序列的编码表达进行对准。
我们会分别给$C_{ind}$和$Q_{ind}$增加一个预训练的嵌入使得现在的$C_{ind} \in \mathbb{R}^{(l+1) \times d}$以及$Q_{ind} \in \mathbb{R}^{(m+1) \times d}$，
这样做可以让每个单词不会和序列中的某个单词强制对准。

令$softmax(X)$表示逐列进行softmax操作，即矩阵$X$中的每列进行标准化后的和为1。
我们将内容信息的序列表示与任务序列表示之间进行点积计算得到相似性得分，并使用归一化操作来获得数据的对准，如公式\ref{cnl2sqleq:encoder3}：
\begin{equation}
  \label{cnl2sqleq:encoder3}
  softmax(C_{ind}Q_{ind}^{\top}) = S_{cq} \in \mathbb{R}^{(l+1) \times (m+1)} \qquad softmax(Q_{ind}C_{ind}^{\top}) = S_{qc} \in \mathbb{R}^{(m+1) \times (l+1)}
\end{equation}

\textbf{对偶协同注意力阶段}

再对准之后，我们要去计算一个序列中的某个单词和另一个序列中相关的单词的加权求和，如公式\ref{cnl2sqleq:encoder4}。
\begin{equation}
  \label{cnl2sqleq:encoder4}
  S_{cq}^{\top}C_{ind} = C_{sum} \in \mathbb{R}^{(m+1) \times d} \qquad S_{qc}^{\top}Q_{ind} = Q_{sum} \in \mathbb{R}^{(l+1) \times d}
\end{equation}

对偶协同注意力就是要通过同样的权重将对准时得到的信息传递给原序列，如公式\ref{cnl2sqleq:encoder5}。
\begin{equation}
  \label{cnl2sqleq:encoder5}
  S_{qc}^{\top}C_{sum} = C_{coa} \in \mathbb{R}^{(l+1) \times d} \qquad S_{cq}^{\top}Q_{sum} = Q_{coa} \in \mathbb{R}^{(m+1) \times d}
\end{equation}

可以发现，进行了加权求和和协同注意力的表示中的第一列为之前我们添加进去的冗余的嵌入。
在这里我们不再需要这一个嵌入，所以将矩阵中的这一列去除掉，得到$C_{coa} \in \mathbb{R}^{l \times d}$和$Q_{coa} \in \mathbb{R}^{m \times d}$。

\textbf{压缩阶段}

为了将对偶协同注意力操作之前的信息压缩至$d$维，我们接着把之前得到的四种序列表示进行拼接并输入一个单独的双向LSTM中，如公式\ref{cnl2sqleq:encoder6}。
\begin{align}
  \label{cnl2sqleq:encoder6}
  BiLSTM_{comC}([C_{proj};C_{ind};Q_{sum};C_{coa}]) =  C_{com} \in \mathbb{R}^{m \times d}\\
  BiLSTM_{comQ}([Q_{proj};Q_{ind};C_{sum};Q_{coa}]) =  Q_{com} \in \mathbb{R}^{m \times d}
\end{align}

\textbf{自注意力阶段}

接着，我们使用一个多头的放缩点积注意力机制（multi-head, scaled dot-product attention）[！！引用！！]来捕获每个序列内部的长距离的依赖性，其表达式为公式\ref{cnl2sqleq:encoder7}。

\begin{gather}
  \label{cnl2sqleq:encoder7}
  Attention(\widetilde{X},\widetilde{Y},\widetilde{Z}) = softmax\{\frac{\widetilde{X}\widetilde{Y}^{\top}}{\sqrt{d}}\} \widetilde{Z}\\
  MultiHead(\widetilde{X},\widetilde{Y},\widetilde{Z}) = [h_1;\cdots;h_p]W^o \qquad \mbox{其中}h_j= Attention(\widetilde{X}W^{\widetilde{X}}_j,\widetilde{Y}W^{\widetilde{Y}}_j,\widetilde{Z}W^{\widetilde{Z}}_j)
\end{gather}

公式\ref{cnl2sqleq:encoder7}中变换都为线性变换，所以多头注意力机制可以维持$d$维不变，有公式\ref{cnl2sqleq:encoder8}：
\begin{equation}
  \label{cnl2sqleq:encoder8}
  MultiHead_C(C_{com},C_{com},C_{com}) = C_{mha} \qquad MultiHead_Q(Q_{com},Q_{com},Q_{com}) = Q_{mha}
 \end{equation}

之后，我们使用了一个残差前馈网络（feedforward networks，简称FFN）如公式\ref{cnl2sqleq:encoder9}，其中$U \in \mathbb{R}^{d \times f}$，$V \in \mathbb{R}^{f \times d}$。其激活函数为Relu函数[！！引用！！],并且在输入和输出都使用了正则化(layer normalization)[！！引用！！]。
\begin{gather}
  \label{cnl2sqleq:encoder9}
  FFN(X) = \max(0,XU)V + X\\
  FFN_C(C_{com} + C_{mha}) = C_{self} \in \mathbb{R}^{l \times d} \qquad FFN_Q(Q_{com} + Q_{mha}) = Q_{self} \in \mathbb{R}^{m \times d}
\end{gather}

\textbf{最终编码阶段}

最后，我们使用两个双向LSTM将所有信息进行汇总并将矩阵$C_{fin}$和$Q_{fin}$输入给解码器，如公式\ref{cnl2sqleq:encoder10}。
\begin{equation}
  \label{cnl2sqleq:encoder10}
  BiLSTM_{finC}(C_{self}) = C_{fin} \in \mathbb{R}^{l \times d} \qquad BiLSTM_{finQ}(Q_{self}) = Q_{fin} \in \mathbb{R}^{m \times d} 
\end{equation}


\subsection{解码器}
\label{cnl2sql:decoder}

在编码器得到最终的表示$C_{fin} \in \mathbb{R}^{l \times d}$和$Q_{fin} \in \mathbb{R}^{m \times d}$（即编码之后的内容信息和任务信息）后进入到解码器解码。
在下文中，我们将对解码器的每个阶段进行进一步的说明。

\textbf{结果表示阶段}

在训练阶段，解码器首先要将结果$a$的表示投射到$d$维的空间上，如公式\ref{cnl2sqleq:decoder1}

\begin{equation}
  \label{cnl2sqleq:decoder1}
  AW_2 = A_{proj} \in \mathbb{R}^{n \times d} 
\end{equation}

之后紧接的是一个自注意力层，该注意力层与编码器中的自注意力层相对应。
由于该过程不包含循环和卷积操作，所以我们为$A_{proj}$加上了一个位置编码信息$x$，如公式\ref{cnl2sqleq:decoder2}。
\begin{equation}
  \label{cnl2sqleq:decoder2}
  A_{proj} + PE = A_{ppr} \in \mathbb{R}^{n \times d},\qquad \mbox{其中} PE[t,k] = \{   
  \begin{array}{lr}
    sin(\frac{t}{10000^{\frac{k}{2d}}}) &  k\mbox{为偶数}\\
    cos(\frac{t}{10000^{\frac{k-1}{2d}}}) &  k\mbox{为奇数}
  \end{array}
\end{equation}

\textbf{自注意力阶段}
接着我们使用了自注意力机制[！！引用！！]从而使得解码器能够知道之前的输出（若之前没有输出则认为之前的输出为某个初始化的单词）和相关内容是什么，从而产生下一个输出。
在多头的注意力机制之后使用了一个残差前馈网络（FFN，其定义在\ref{cnl2sql:encoder}节中的自注意力阶段给出），如公式\ref{cnl2sqleq:decoder3}。

\begin{gather}
  \label{cnl2sqleq:decoder3}
  MultiHead_A(A_{ppr},A_{ppr},A_{ppr}) = A_{mha} \in \mathbb{R}^{n \times d}\\
  MultiHead_{A}C((A_{mha} + A_{ppr}),C_{fin},C_{fin}) = A_{ac} \in \mathbb{R}^{n \times d}\\
  FFN_A(A_{ac} + A_{mha} + A_{ppr}) = A_{self} \in \mathbb{R}^{n \times d}
\end{gather}

\textbf{获得中间状态阶段}

之后，我们使用了一个标准的带注意力机制的LSTM网络来对每个时刻$t$产生一个内容状态$\widetilde{c}_t$。
这个LSTM又将使用之前的结果序列中的单词$A^{t-1}_{self}$和内容状态来生成一个中间状态$h_t$，如公式\ref{cnl2sqleq:decoder4}。

\begin{equation}
  \label{cnl2sqleq:decoder4}
  LSTM([(A_{self})_{t-1};\widetilde{c}_{t-1}],h_{t-1})= h_t \in \mathbb{R}^{d} 
\end{equation}

\textbf{任务与内容注意力阶段}

上一阶段中的中间状态可用来获得注意力权重$\alpha^C_t$以及$\alpha^Q_t$Q，解码器从而可以获得$t$时刻的编码信息，如公式\ref{cnl2sqleq:decoder5}。

\begin{equation}
  \label{cnl2sqleq:decoder5}
  softmax(C_{fin}(W_2 h_t)) = \alpha^C_t \in \mathbb{R}^{l} \qquad softmax(Q_{fin}(W_3 h_t)) = \alpha^Q_t \in \mathbb{R}^{m}
\end{equation}

\textbf{获得内容与任务状态阶段}

内容的表示将于这些权重相结合并输入给一个前馈网络从而形成内容状态与任务状态，激活函数为$tanh$，如公式\ref{cnl2sqleq:decoder6}。
\begin{equation}
  \label{cnl2sqleq:decoder6}
  tanh(W_4[C^{\top}_{fin}\alpha^C_t;h_t] = \widetilde{c}_{t} \in \mathbb{R}^{d} \qquad tanh(W_5[Q^{\top}_{fin}\alpha^Q_t;h_t] = \widetilde{q}_{t} \in \mathbb{R}^{d}
\end{equation}

\textbf{多指针生成阶段}

由于我们的模型必须要能够生成那些未出现在任务或内容序列中的单词，所以需要额外声明一个词汇表$v$。
我们会从任务、内容和外部的词汇表中分别获得每个单词的概率分布，如公式\ref{cnl2sqleq:decoder7}。

\begin{gather}
  \label{cnl2sqleq:decoder7}
  \sum_{i:c_i=w_t} (\alpha^C_t)_i = p_c(w_t) \in \mathbb{R}^{n}\\
  \sum_{i:q_i=w_t} (\alpha^Q_t)_i = p_q(w_t) \in \mathbb{R}^{m}\\
  softmax(W_v\widetilde{c}_t) = p_v(w_t) \in \mathbb{R}^{v}
\end{gather}

这些概率分布将会覆盖任务、内容和外部词汇表中所有单词的集合，其中却是的一些条目将会被赋为0值从而使所有分布都在$\mathbb{R}^{l+m+v}$。
最后，我们设置两个标量开关来调节每个分布的重要程度并确认最终输出的分布，如公式\ref{cnl2sqleq:decoder8}。
\begin{gather}
  \label{cnl2sqleq:decoder8}
  \sigma (W_{pv}[\widetilde{c}_t;h_t;(A_{self})_{t-1}]) = \gamma \in [0,1]\\
  \sigma (W_{cq}[\widetilde{q}_t;h_t;(A_{self})_{t-1}]) = \lambda \in [0,1]\\
  \gamma p_v(w_t) + (1 - \gamma)[\lambda p_c(w_t) + (1 - \lambda)p_q(w_t)] = p(w_t) \in \mathbb{R}^{l+m+v}
\end{gather}

需要说明的是，在训练的全时域中，我们采用的是单词级别的对数似然损失，如公式\ref{cnl2sqleq:decoder9}。

\begin{equation}
  \label{cnl2sqleq:decoder9}
  \mathcal L = -\sum^T_t \log p(a_t)
\end{equation}


\section{实验与分析}
\label{cnl2sql:syyfx}
\subsection{多任务网络的实验结果}

\begin{table}[!htpb]
  \bicaption[出现在表目录的标题]
    {多任务网络的实验结果}
    {A Table with footnotes}
  \label{tab:drwwldsyjg}
  \centering
  \begin{threeparttable}[b]
     \begin{tabular}{cccccccccc}
      \toprule
      \multirow{2}{18mm}{数据集}&\multicolumn{4}{c}{单任务} & \multicolumn{5}{c}{多任务}\\
      \cmidrule(lr){2-5}\cmidrule(lr){6-10}
      & S2S & w/SAtt & +CAtt & +QPtr & S2S & w/SAtt & +CAtt & +QPtr & +ACurr\\
      \midrule
      % 增强解析器（静态预言） & 87.1 & 88.2 & 65.9 & 67.5\\
      % 增强解析器（非确定性预言, 不解决过滤条件顺序问题） & 88.1 & 89.2 & 68.3 & 69.1\\
      SQuAD & 48.2 & 68.2 & 74.6 & 75.5 & 47.5 & 66.8 & 71.8 & 70.8 & 74.3\\
      IWSLT & 25.0 & 23.3 & 26.0 & 25.5 & 14.2 & 13.6 & 9.0 & 16.1 & 13.7\\
      CNN/DM & 19.0 & 20.0 & 25.1 & 24.0 & 25.7 & 14.0 & 15.7 & 23.9 & 24.6\\
      MNLI & 67.5 & 68.5 & 34.7 & 72.8 & 60.9 & 69.0 & 70.4 & 70.5 & 69.2\\
      SST & 86.4 & 86.8 & 86.2 & 88.1 & 85.9 & 84.7 & 86.5 & 86.2 & 86.4\\
      QA-SRL & 63.5 & 67.8 & 74.8 & 75.2 & 68.7 & 75.1 & 76.1 & 75.8 & 77.6\\
      QA-ZRE & 20.0 & 19.9 & 16.6 & 15.6 & 28.5 & 31.7 & 28.5 & 28.0 & 34.7\\
      WOZ & 85.3 & 86.0 & 86.5 & 84.4 & 84.0 & 82.8 & 75.1 & 80.6 & 84.1\\
      WikiSQL & 60.0 & 72.4 & 72.3 & 72.6 & 45.8 & 64.8 & 62.9 & 62.0 & 58.7\\
      MWSC & 43.9 & 46.3 & 40.4 & 52.4 & 52.4 & 43.9 & 37.8 & 48.8 & 48.4\\
      \midrule
      总得分 & - & - & - & - & 473.6 & 546.4 & 533.8 & 562.7 & 571.7\\
      \bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table}

如图\ref{fig:cnl2sqlnet}以及\ref{fig:cnl2sqltcr}所示，我们的模型的训练样本为包含任务序列、内容序列以及结果序列的三元组。
我们所做的对比实验的基准为使用了指针生成器的序列到序列的模型（简称S2S模型）[！！引用！！]。
不一样的是，S2S模型的输入为自然语言问题序列，而多任务网络的输入为任务及内容序列的集合。

实验结果如表\ref{cnl2sql:syyfx}所示，表中S2S代表序列到序列(sequence to sequence)模型，(w/SAtt)代表增加带自注意力机制的S2S模型，
(+CAtt)代表将任务与内容序列拆分为两个输入序列并增加对偶协同注意力机制，(+QPtr)代表增加任务指针器， (+ACurr)代表增加反递进学习过程。
总得分表示多任务学习时各个子任务得分的和，由于单任务不存在多任务的得分和所以标记为’-’。


在表\ref{cnl2sql:syyfx}中的指标可以看出，S2S模型在SQuAD任务上的效果较差，在WikiSQL数据集上的得分比之前的序列到序列的基准方案要高，但是普遍都没有多任务网络（+QPtr）的效果优异。
在S2S模型的基础上增加了带自注意力机制层（w/SAtt）的编码器和解码器之后[！！引用！！]，模型具备了整合来自任务和内容信息的能力。
从表中可以看出，新的模型在SQuAD任务上提升了20 nF1，QA-SRL任务上提升了4 nF1，WikiSQL上提升了12 LFEM。
模型在WikiSQL上的表现接近了最高水平的范围，在没有使用结构化的方法[！！引用！！]下达到了72.4\%。

接着，我们尝试将任务和内容从输入中进行拆分并增加了对偶协同注意力机制（+CAtt）。
可以看到，模型在SQuAD和QA-SRL上的性能提升了超过5 nF1，但是在其他任务上没有明显的提升，甚至在MNLI和MWSC任务上的性能有所降低。
其原因是由于两个任务往往能够从输入中直接复制一些单词作为输出，S2S模型的输入为任务和内容信息的集合，所以其中的指针生成器可以直接完成从输入中复制信息到输出的操作。
而将任务和内容信息拆分为两个输入序列的时候，模型就失去了这个能力。

为了解决这个问题，我们在之前的模型上又增加了一个任务指针（+QPtr）进行实验。
实验结果显示，任务指针有效的提高了模型在MNLI和MWSC上的性能。
同时，模型在SQuAD任务上提升到了75.5 nF1，达到了SQuAD领域中使用直接跨度检测方法的水平，也就是说多任务网络达到了不明确将问题进行跨度检测的方法中的最高水平。

最后的模型在WikiSQL任务上达到了72.4\%的lfEM以及80.4\%的数据库执行准确率，超过了之前的最高水平[！！引用！！]的71.7\%和78.5\%。

在单任务与多任务的比较中，我们也得到了类似的结果以及注意到了一些额外的特点。
多任务与单任务模型相比，在QA-ZRE任务上的F1得分又有11点提升，也验证了多任务的模型在解决零样本学习（zero-shot learning）上的能力。

\subsection{不同优化策略下的实验结果}

在多任务的训练过程中我们尝试了各种批量采样（batch-level sampling）的策略，包括联合训练、递进学习、反递进学习等，其实验结果如表\ref{tab:drwwlzbtxlclxdsyjg}所示。

我们首先考虑的是完全联合（fully joint）的策略。
从训练开始到结束，我们对所有的任务按照固定的顺序循环的进行批量采样。
这种策略在迭代（iteration）次数较少的单任务下的表现良好，见表\ref{tab:drwwlzbtxlclxdsyjg}，但在迭代次数较多的单任务下性能不佳。
经过总结我们发现，单任务与多任务场景下的性能的差异与单任务的迭代次数之间有很强的关联。

由于上述原因，我们还尝试了一些反递进学习的策略[！！引用！！]。
这些训练的策略都是由两阶段组成的：1）在第一阶段中对部分任务进行联合训练（往往选择较难的任务）。2）第二阶段根据完全联合策略进行对所有任务进行训练。

首先，我们尝试了先将SQuAD任务进行单独训练，之后再对其他的所有任务进行完全联合训练。
由于我们的多任务网络对于所有任务来说是一种基于问答的方法，所以我们联想到是否可以先与训练SQuAD这样的QA数据集然后再对其他的类QA的任务进行训练。
这种方法可以使得解码器先学会如何从内容信息中检索出有用信息，然后学习不同的任务以及如何产生属于不同任务领域的单词。
在SQuAD这个任务场景下进行预训练已经被证明可以提高NLI的性能[！！引用！！]。
根据我们的实验来看，这种方式相对于其他的训练策略而言是一种更有效的训练策略，在总得分上获得了571.7的最高分。
需要特别提到的是，这种策略在IWSLT任务上的得分较低，但是在其他的任务中会有更大的提升，特别是使用指针的任务。

为了探索在递进学习策略的初始状态下增加额外的任务是否会进一步提高性能，我们还尝试了将IWSLT、CNN/DM任务和IWSLT、CNN/DM、MNLI任务加入到第一阶段进行训练。
这些任务包含了与其他任务非常相关的训练样本，并且也包含了最长的结果序列。
除此之外，这些任务具有很高的多样性，应该能够让解码器学会更多的解码方式，例如针对IWSLT的词汇表、针对SQuAD与CNN/DM的内容指针以及针对MNLI的任务指针。
然而，实验结果表明这些新增的任务并没有提升模型的性能。
在将SQuAD, IWSLT, CNN/DM和MNLI加入到递进学习的初始状态下时，其他的一些任务（例如QA-SRL, WikiSQL和MWSC）的性能明显下降。

另外，我们也尝试了在递进学习策略的初始状态下使用了SST, QA-SRL, QA-ZRE, WOZ, WikiSQL和MWSC，这也就意味着先选择一些最简单的任务并进行训练。
实验证明，这个策略是一个非常低劣的策略，不仅在初始递进学习中的任务的性能很差，在SQuAD和IWSLT上的表现更差。

事实证明，对于我们的任务而言，使用反递进学习策略的效果优于递进学习策略，也就是说明了先学习简易的任务未能使模型学到能应用于其他任务的表示，甚至使得模型想要学习到其他任务有用的内部表示变得更加地困难。
除此之外，实验的结果也说明了多任务学习的挑战。
通过改变所学习的任务的先后次序可能可以提高某些任务性能，但也很有可能使其他的任务的性能下降。
这也是多任务学习中一个非常值得研究的任务。


\begin{table}[!htpb]
  \bicaption[出现在表目录的标题]
    {多任务网络在不同训练策略下的实验结果}
    {A Table with footnotes}
  \label{tab:drwwlzbtxlclxdsyjg}
  \centering
  \begin{threeparttable}[b]
     \begin{tabular}{cccccc}
      \toprule
      \multirow{2}{18mm}{数据集}&\multirow{2}{18mm}{完全联合}&\multirow{2}{18mm}{递进} & \multicolumn{3}{c}{反递进}\\
      \cmidrule(lr){4-6}
      % & S2S & w/SAtt & +CAtt & +QPtr & S2S & w/SAtt & +CAtt & +QPtr & +ACurr\\
      & & & SQuAD & +IWSLT+CNN/DM & +MNLI\\
      \midrule
      SQuAD & 70.8 & 43.4 & 74.3 & 74.5 & 74.6\\
      IWSLT & 16.1 & 4.3 & 13.7 & 18.7 & 19.0\\
      CNN/DM & 23.9 & 21.3 & 24.6 & 20.8 & 21.6\\
      MNLI & 70.5 & 58.9 & 69.2 & 69.6 & 72.7\\
      SST & 86.2 & 84.5 & 86.4 & 83.6 & 86.8\\
      QA-SRL & 75.8 & 70.6 & 77.6 & 77.5 & 75.1\\
      QA-ZRE & 28.0 & 24.6 & 34.7 & 30.1 & 37.7\\
      WOZ & 80.6 & 81.9 & 84.1 & 81.7 & 85.6\\
      WikiSQL & 62.0 & 68.6 & 58.7 & 54.8 & 42.6\\
      MWSC & 48.8 & 41.5 & 48.4 & 34.9 & 41.5\\
      \midrule
      % 总得分 & - & - & - & - & 473.6 & 546.4 & 533.8 & 562.7 & 571.7\\
      总得分 & 562.7 & 499.6 & 571.7 & 546.2 & 557.2\\
      \bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table}

\subsection{实验分析}

\textbf{多指针生成器与任务识别}

多任务网络会从三个方面进行选择：从词汇表生成词汇、任务序列上的指针以及内容序列上的指针。
尽管模型没有明确的监督信号来训练这些选择，但网络仍可以学会如何在这三个选择之间进行切换。
图xx显示了我们最好的模型选择每个选项的频率是怎样的。
可以看出，对于SQuAD，QA-SRL和WikiSQL任务，模型主要是从内容信息中进行选择和复制，这正是因为这些任务中的正确结果主要攘括在内容中。
由于答案的摘要主要由内容中的单词组成，模型在CNN/DM任务中也是主要从内容中进行复制。
对于SST，MNLI和MWSC，由于任务序列中会包含需要的类别，所以模型也偏向选择任务指针。
对于IWSLT和WOZ，模型更喜欢选择从词汇表生成单词，因为中文的单词和对话的状态字段很少在对应的内容信息中。

同时，使用了TCR模板之后的所有任务之间不会使得模型产生混淆。例如：中文单词仅仅会在中文翻译成英文的过程中出现，而在情感分析任务中也从未输出过非“正面”或“负面”的情感类别。

\textbf{模型的拓展性}

多任务网络具有很好的拓展性，除了中文翻译为英文任务和英文生成SQL自然语言任务外对图\ref{cnl2sql:syyfx}中的其他任务都有很好的表现。
在这十个任务共同学习的模型的基础上进行其他任务的学习更加表现了模型良好的拓展性。
对于两个新的任务，英文到捷克语的翻译任务以及命名实体识别（NER）上，使用训练好的模型进行微调可以在更少的迭代次数下达到模型的收敛，达到很好的效果，如图xx。
其中，英文到捷克语的翻译任务使用的是En->Cs数据集，NER任务使用的是OntoNotes 5.0数据集[！！引用！！]。

\textbf{文本分类的零样本学习}

由于MNLI包含在是个任务中，因此模型也可以适应斯坦福自然语言推理语料库（SNLI）[！！引用！！]。
在预训练的多任务网络之上进行微调可以达到87％的精确匹配分数，比随机初始化再训练提升了2分，也比现有的最高水平提升了了2分[！！引用！！]。
更值得注意的是，在没有对SNLI进行任何微调的情况下，在原模型上预训练的多任务网络仍然达到了62％的精确匹配分数。
因为十个任务包含SST任务，所以它也可以在其他二元情感分类任务上表现良好。 
在亚马逊和Yelp评论数据集[！！引用！！]上，预训练的多任务网络分别达到了82.1\%和80.8\%的精确匹配分数并且不需要对模型进行任何微调。
这些结果表明在多任务网络模型的基础上进行微调可以是的模型拓展到域外的问题上，甚至可以适应更广的文本分类问题上。

\section{本章小结}
