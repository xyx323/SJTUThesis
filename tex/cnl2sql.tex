%# -*- coding: utf-8-unix -*-
% !TEX program = xelatex
% !TEX root = ../thesis.tex
% !TEX encoding = UTF-8 Unicode
%%==================================================
%% chapter02.tex for SJTU Master Thesis
%% based on CASthesis
%% modified by wei.jianwen@gmail.com
%% Encoding: UTF-8
%%==================================================

\chapter{基于多任务学习的NL2SQL生成}
\label{chap:cnl2sql}

\section{研究问题}
\section{相关技术}
\section{解决方案}
\section{实验与分析}
\section{本章小结}
Machine  ComprehensionQuestion answering (QA) mod-els receive a question and a context that contains informationnecessary  to  output  the  desired  answer.  We  use  the  StanfordQuestion  Answering  Dataset  (SQuAD)  []  for  this  task.  Con-texts  are  paragraphs  taken  from  the  English  Wikipedia,  andanswers  are  sequences  of  words  copied  from  the  context.SQuAD  uses  a  normalized  F1  (nF1)  metric  that  strip  outarticles and punctuation.Machine Translation.Machine translation models receivean  input  document  in  a  source  language  that  must  be  trans-lated  into  a  target  language.  We  use  the  2016  English  to

German training data prepared for the International Workshopon  Spoken  Language  Translation  (IWSLT)  [].  Examples  arefrom transcribed TED presentations that cover a wide varietyof  topics  with  conversational  language.  We  evaluate  with  acorpus-level BLEU score [] on the 2013 and 2014 test sets asvalidation and test sets, respectively.Natural  Language  Inference.Natural  Language  Infer-ence  (NLI)  models  receive  two  input  sentences:  a  premiseand  a  hypothesis.  Models  must  then  classify  the  inferencerelationship between the two as one of entailment, neutrality,or  contradiction.  We  use  the  Multi-Genre  Natural  LanguageInference Corpus (MNLI) [] which provides training examplesfrom  multiple  domains  (transcribed  speech,  popular  fiction,government  reports)  and  test  pairs  from  seen  and  unseen

domains.  MNLI  uses  an  exact  match  (EM)  score.  SentimentAnalysis.  Sentiment  analysis  models  are  trained  to  classifythe  sentiment  expressed  by  input  text.  The  Stanford  Senti-ment  Treebank  (SST)  []  consists  of  movie  reviews  with  thecorresponding  sentiment  (positive,  neutral,  negative).  We  usethe unparsed, binary version []. SST also uses an EM score.Semantic  Parsing.SQL  query  generation  is  related  tosemantic  parsing.  Models  based  on  the  WikiSQL  dataset[]  translate  natural  language  questions  into  structured  SQLqueries  so  that  users  can  interact  with  a  database  in  naturallanguage. WikiSQL is evaluated by a logical form exact match(lfEM)  to  ensure  that  models  do  not  obtain  correct  answersfrom incorrectly generated queries.Summarization.Summarization models take in a documentand  output  a  summary  of  that  document.  Most  important  torecent  progress  in  summarization  was  the  transformation  ofthe CNN/DailyMail (CNN/DM) corpus [Hermann et al., 2015]into  a  summarization  dataset  [Nallapati  et  al.,  2016].  We  in-clude the non-anonymized version of this dataset in decaNLP.On  average,  these  examples  contain  the  longest  documentsin decaNLP and force models to balance extracting from thecontext  with  generation  of  novel,  abstractive  sequences  ofwords.  CNN/DM  uses  ROUGE-1,  ROUGE-2,  and  ROUGE-L  scores  [Lin,  2004].  We  average  these  three  measures  tocompute an overall ROUGE score.Sentiment Analysis.Sentiment analysis models are trainedto classify the sentiment expressed by input text. The StanfordSentiment  Treebank  (SST)  [Socher  et  al.,  2013]  consists  ofmovie  reviews  with  the  corresponding  sentiment  (positive,neutral, negative). We use the unparsed, binary version [Rad-ford et al., 2017]. SST also uses an EM score.Semantic  Role  Labeling.Semantic  role  labeling  (SRL)models  are  given  a  sentence  and  predicate  (typically  a  verb)and must determine who did what to whom, when, and where[Johansson  and  Nugues,  2008].  We  use  an  SRL  dataset  thattreats  the  task  as  question  answering,  QA-SRL  [He  et  al.,2015]. This dataset covers both news and Wikipedia domains,but  we  only  use  the  latter  in  order  to  ensure  that  all  datafor decaNLP can be freely downloaded. We evaluate QA-SRLwith the nF1 metric used for SQuAD.Relation Extraction.Relation extraction systems take in apiece  of  unstructured  text  and  the  kind  of  relation  that  is  tobe extracted from that text. In this setting, it is important thatmodels can report that the relation is not present and cannot beextracted. As with SRL, we use a dataset that maps relations toa set of questions so that relation extraction can be treated asquestion answering: QA-ZRE [Levy et al., 2017]. Evaluationof  the  dataset  is  designed  to  measure  zero  shot  performanceon new kinds of relations   the dataset is split so that relationsseen  at  test  time  are  unseen  at  train  time.  This  kind  of  zero-shot relation extraction, framed as question answering, makesit  possible  to  generalize  to  new  relations.  QA-ZRE  uses  acorpus-level  F1  metric  (cF1)  in  order  to  accurately  accountfor unanswerable  questions. This F1  metric  defines precisionas  the  true  positive  count  divided  by  the  number  of  timesthe  system  returned  a  non-null  answer  and  recall  as  the  true

positive  count  divided  by  the  number  of  instances  that  havean answer.Goal-Oriented   Dialogue.Goal-Oriented  Dialogue.  Dia-logue  state  tracking  is  a  key  component  of  goal-orienteddialogue  systems.  Based  on  user  utterances,  actions  takenalready, and conversation history, dialogue state trackers keeptrack of which predefined goals the user has for the dialoguesystem  and  which  kinds  of  requests  the  user  makes  as  thesystem  and  user  interact  turn-by-turn.  We  use  the  EnglishWizard of Oz (WOZ) restaurant reservation task [Wen et al.,2016],  which  comes  with  a  predefined  ontology  of  foods,dates, times, addresses, and other information that would helpan agent make a reservation for a customer. WOZ is evaluatedby turn-based dialogue state EM (dsEM) over the goals of thecustomers.Semantic   Parsing.SQL  query  generation  is  related  tosemantic  parsing.  Models  based  on  the  WikiSQL  dataset[Zhong et al., 2017] translate natural language questions intostructured  SQL  queries  so  that  users  can  interact  with  adatabase  in  natural  language.  WikiSQL  is  evaluated  by  alogical  form  exact  match  (lfEM)  to  ensure  that  models  donot obtain correct answers from incorrectly generated queries.Pronoun  Resolution.Our final task is based on Winogradschemas  [Winograd,  1972],  which  require  pronoun  resolu-tion:  ”Joan  made  sure  to  thank  Susan  for  the  help  she  had[given/received].  Who  had  [given/received]  help?  Susan  orJoan?”.  We  started  with  examples  taken  from  the  WinogradSchema Challenge [Levesque et al., 2011] and modified themto  ensure  that  answers  were  a  single  word  from  the  context.This modified Winograd Schema Challenge (MWSC) ensuresthat  scores  are  neither  inflated  nor  deflated  by  oddities  inphrasing  or  inconsistencies  between  context,  question,  andanswer. We evaluate with an EM score.The  Decathlon  Score  (decaScore).Models  competing  ondecaNLP are evaluated using an additive combination of eachtask-specific metric. All metrics fall between 0 and 100, so thatthe decaScore naturally falls between 0 and 1000 for ten tasks.Using  an  additive  combination  avoids  issues  that  arise  fromweighing different metrics. All metrics are case insensitive.As shown in Table II. All metrics are case insensitive. nF1is  the  normalized  F1  metric  used  by  SQuAD  that  strips  outarticles  and  punctuation.  EM  is  an  exact  match  comparison:for  text  classification,  this  amounts  to  accuracy;  for  WOZ  itis equivalent to turn-based dialogue state exact match (dsEM)and  for  WikiSQL  it  is  equivalent  to  exact  match  of  logicalforms (lfEM). F1 for QA-ZRE is a corpus level metric (cF1)that  takes  into  account  that  some  question  are  unanswerable.Precision is the true positive count divided by the number oftimes the system returned a non-null answer. Recall is the truepositive  count  divided  by  the  number  of  instances  that  havean answer.

Because  every  task  is  framed  as  question  answering  andtrained jointly, we call our model a multitask question answer-

ing  network  (MQAN).  Each  example  consists  of  a  context,question,  and  answer  as  shown  in  Fig.  1.  Many  recent  QAmodels  for  question  answering  typically  assume  the  answercan  be  copied  from  the  context  [Wang  and  Jiang,  2017,  Seoet  al.,  2017,  Xiong  et  al.,  2018],  but  this  assumption  doesnot  hold  for  general  question  answering.  The  question  oftencontains  key  information  that  constrains  the  answer  space.Noting this, we extend the coattention of [Xiong et al., 2017]to enrich the representation of not only the input but also thequestion.  Also,  the  pointer-mechanism  of  [See  et  al.,  2017]is generalized into a hierarchical, multi-pointer-generator thatenables  the  capacity  to  copy  directly  from  the  question  andthe context.During training, the MQAN takes as input three sequences:a  context  c  with  l  tokens,  a  question  q  with  m  tokens,  andan answer a with n tokens. Each of these is represented by amatrix where the ith row of the matrix corresponds to a demb-dimensional embedding (such as word or character vectors) forthe ith token in the sequence:

Encoder:
xxxxx
Decoder:
xxxxx